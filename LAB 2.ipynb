{
 "cells": [
  {
   "cell_type": "raw",
   "id": "dedab71d",
   "metadata": {},
   "source": [
    "(1) What is Stop Word Removal? Implement it?\n",
    "(2) How to find similarities between two words?\n",
    "(3) What Is Tokenization?\n",
    "(4) How one can tokenize tweets? Show by example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95caa2",
   "metadata": {},
   "source": [
    "# Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210338d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ea88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a39df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fae62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'process', 'of', 'converting', 'data', 'to', 'something', 'a', 'computer', 'can', 'understand', 'is', 'referred', 'to', 'as', 'pre-processing', '.', 'One', 'of', 'the', 'major', 'forms', 'of', 'pre-processing', 'is', 'to', 'filter', 'out', 'useless', 'data', '.']\n",
      "['The', 'process', 'converting', 'data', 'something', 'computer', 'understand', 'referred', 'pre-processing', '.', 'One', 'major', 'forms', 'pre-processing', 'filter', 'useless', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data.\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4fd49",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49df499f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'process', 'of', 'converting', 'data', 'to', 'something', 'a', 'computer', 'can', 'understand', 'is', 'referred', 'to', 'as', 'pre-processing', '.', 'One', 'of', 'the', 'major', 'forms', 'of', 'pre-processing', 'is', 'to', 'filter', 'out', 'useless', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data.\"\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba646b81",
   "metadata": {},
   "source": [
    "# tokenize tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930780b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'an', 'honor', 'to', 'welcome', 'United', 'Kingdom', 'Prime', 'Minister', 'Boris', 'Johnson', 'to', 'the', 'White', 'House', 'this', 'afternoon', '.', 'The', 'bond', 'between', 'our', 'two', 'nations', 'is', 'ironclad', 'and', 'we', '’', 're', 'committed', 'to', 'working', 'together', 'on', 'everything', 'from', 'climate', 'change', 'to', 'COVID-19', 'in', 'the', 'years', 'ahead', '.']\n"
     ]
    }
   ],
   "source": [
    "tweet_text = '''\n",
    "It was an honor to welcome United Kingdom Prime Minister\n",
    "Boris Johnson to the White House this afternoon. The bond between our two\n",
    "nations is ironclad and we’re committed to working together on everything\n",
    "from climate change to COVID-19 in the years ahead.\n",
    "'''\n",
    "tweet_tokens = nltk.word_tokenize(tweet_text)\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda7726",
   "metadata": {},
   "source": [
    "# Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b42727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter two space-separated words\n",
      "man woman\n",
      "man True 6.352939 False\n",
      "woman True 6.8987513 False\n",
      "Similarity: 0.7401744\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "## Loading the small model containing tensors.\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "  \n",
    "print(\"Enter two space-separated words\")\n",
    "words = input()\n",
    "  \n",
    "tokens = nlp(words)\n",
    "  \n",
    "for token in tokens:\n",
    "    # Printing the following attributes of each token.\n",
    "    # text: the word string, has_vector: if it contains\n",
    "    # a vector representation in the model, \n",
    "    # vector_norm: the algebraic norm of the vector,\n",
    "    # is_oov: if the word is out of vocabulary.\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "  \n",
    "token1, token2 = tokens[0], tokens[1]\n",
    "  \n",
    "print(\"Similarity:\", token1.similarity(token2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
